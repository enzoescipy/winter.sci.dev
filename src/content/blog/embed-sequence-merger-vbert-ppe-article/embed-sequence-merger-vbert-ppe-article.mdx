---
author: WINTER.SCI.DEV
pubDatetime: 2025-10-26T00:05:00.000+09:00
title: "vBERT: re-embed the embedding? The `Vector` Sequence Merger Transformer Synthesizer"
slug: embed-sequence-merger-vbert-ppe-article
featured: true
draft: false
tags:
  - AI
  - NLP
  - vBERT
  - PPE
  - Embed Sequence Merger
  - FINESSE
description: "The Sequence Merger is a custom neural architecture designed for intelligent merging of embedding vector sequences into a single representative vector."
---


import { Image } from 'astro:assets';
import LinkButton from '@components/LinkButton.astro';
import CaptionCenteredImage from '@components/CaptionCenteredImage.astro';
import { LinkPreview } from 'astro-embed';

import coffeeAndTimeImage from "./source/coffee-and-time.jpg";
import robotReadingImage from "./source/robot-reading.jpg";
import filmMovieImage from "./source/film-movie.jpg";
import dataFlowImage from "./source/data-flow.jpg";
import debateImage from "./source/debate.jpg";
import colaborateImage from "./source/colaborate.jpg";
import writeImage from "./source/write.jpg";
import oldBooksImage from "./source/old_books.jpg";
import childImage from "./source/child.jpg";

<CaptionCenteredImage 
  imageSrc={coffeeAndTimeImage}
  caption="Time flows with coffee."
  imageAlt="Time flows with coffee."
/>


### In long conversations, the initial flavor of our discussion often fades.

For example, a deep chat with a friend over a cup of coffee—that special aroma, a blend of initial sourness and sweetness—becomes abstracted into 'just a delicious coffee' over time. We constantly try to remember not to lose context in the flow of memory, but sometimes a single broken link can unravel the whole thing.
But does the subtle memory of the harmonious coffee flavor simply fade and blur?

**Absolutely not.**

I believe humans possess a unique power that AI does not: the ability to **forget beautifully.**

Humans forget. But at the same time, they compress memories and faintly connect them. The impression of a nuanced coffee aroma may fade, but an accidentally caught similar scent can instantly bring back memories of old friends met while enjoying coffee. **This is the flexibility of human memory.**

## ChatGPT Has No Memory: The Transformer Decoder Architecture

<CaptionCenteredImage 
  imageSrc={robotReadingImage}
  caption="Machine learning is everything in AI, but ironically, AI is inherently incapable of learning."
  imageAlt="Machine learning is everything in AI, but ironically, AI is inherently incapable of learning."
/>

**Haven't you noticed AI getting smarter at some point?**

The starting point for everything was a single paper published by Google:

<blockquote>
[Attention Is All You Need](https://arxiv.org/abs/1706.03762) — Literally, "*If you only make the program pay attention, it will become AI*."
</blockquote>

This somewhat aggressive assertion subsequently plunged the entire world into an LLM frenzy.<sup><a href='#cite-ref-1' class='inline-link'>1</a></sup><sup><a href='#cite-ref-2' class='inline-link'>2</a></sup> Various generative AIs like ChatGPT, Gemini, and Sonnet originated from this paper.

Its principle is by no means simple. However, to put it simply, they can be seen as playing a game of "who can add text most naturally." ~~(?)~~

<CaptionCenteredImage 
  imageSrc={writeImage}
  caption="Have you ever played a relay story writing game?"
  imageAlt="Have you ever played a relay story writing game?"
/>

"*Once upon a time...*" → "*...in a small village...*" → "*...a boy lived.*"

But what if someone suddenly created a new rule in the middle, saying, "**But actually, the protagonist was a fairy!"** and threw it in unexpectedly?

The people who follow would be confused but would have no choice but to follow that rule. And then they would also naturally adapt, saying, "**The protagonist, who was a fairy, had no choice but to return to the fairy world.**" Do you feel that sense of finding stability?

**Doesn't it somehow feel like our conversations with ChatGPT?**

This process is called "Attention." Each word looks at others (Attention!) and calculates the strength of their relationship, saying, "You are so important to me."

This is half of the paper and the fundamental principle behind LLMs that have captivated the world: **the "Transformer Decoder."**

<blockquote>
They are simply **"text-adding machines."**
</blockquote>

However, because the length of that "subsequent text" is equivalent to dozens of encyclopedias, it is possible for them to act as if they have memory. But then, this question arises:

<blockquote>
Is there a limit to the length of "subsequent text"? If so, where do sentences that exceed that limit go?
</blockquote>

The answer is clear:

**They disappear. Completely.**

## Stories Have Ends

Let's think about relay storytelling again. If the story gets too long, and you think, "*Oh, what did I write at the beginning?*" and try to re-read the first page, wouldn't that page already have been passed on to someone else? **That crucial sentence from the beginning can no longer influence the current flow of the story.**

This is the biggest limitation of the Transformer, the **"Context Window."** AI can only understand the relationships between words that fall within this window. Words outside the window are ignored, as if the first page of a book had been torn out.

**They remember everything *within* the "subsequent text," but nothing *outside* of it.**

<CaptionCenteredImage 
  imageSrc={dataFlowImage}
  caption="With great power comes great cost."
  imageAlt="With great power comes great cost."
/>

The "Attention" of the Transformer architecture is beautiful, but it comes with immense cost. **If there are 10 words, it needs 10x10=100 calculations; if there are 100 words, it needs 100x100=10,000 calculations.** For practical reasons, this calculation range is limited to a certain length. No, it is intentionally trained not to process beyond that limited length. This is a common characteristic of all LLMs today.

<blockquote>
***"I grant you the power to understand all that you see. But come tomorrow, you will forget even what you have seen."***
</blockquote>

## So, is AI forever doomed to the fate of oblivion?

The 'relay storytelling' method we've examined so far was, in fact, **half the solution** proposed by the "Attention Is All You Need" paper. This paper presented two fundamentally different architectures: one was the **Decoder** we saw, and the other was the **Encoder**.

<blockquote>
**If the Decoder is a writer contemplating "what to write," the Encoder is a critic analyzing "what has been written."**
</blockquote>

These two originated from the same root (Attention) but differ entirely in purpose and method. If the Decoder was 'relay storytelling,' the Encoder can be compared to an **'intense book club discussion.'**

### BERT, the forgotten sibling: The Intense Book Club Discussion

<CaptionCenteredImage 
  imageSrc={debateImage}
  caption="The birth of new meaning through passionate discussion—that is the philosophy of BERT."
  imageAlt="The birth of new meaning through passionate discussion—that is the philosophy of BERT."
/>

Picture a book club discussion. All participants read the same book. When the discussion begins, anyone can freely quote any part of the book and delve into its meaning.

*   "That line the protagonist spoke in Chapter 3—doesn't it connect to the first dialogue in Chapter 1?"
*   "The ending is shocking, but that tiny clue in the middle of Chapter 5 was foreshadowing!"

**The Encoder directly implements this principle of a 'book club discussion.'** While the Decoder wrote stories only in one direction (past → future), the Encoder **simultaneously views the beginning, middle, and end of a sentence to grasp the 'true meaning' of every word.** This is the power of **'Bidirectional'**, which the 'B' in BERT signifies.

<blockquote>
**The Encoder sweeps through an entire sentence at once, holistically understanding the relationship each word has with all other words.**
</blockquote>

### Babbling GPT and Silent BERT

As a result, the Encoder BERT does not predict the 'next word.' Instead, it outputs a refined **'Essence of Understanding'**—a cluster of numbers that deeply condenses the meaning of the entire sentence.<sup><a href='#cite-ref-3' class='inline-link'>3</a></sup>

This is not a mere sequence of numbers. It is a **deep insight** into the sentence, including its emotion, nuance, and hidden meanings. The Encoder is an expert at 'understanding' sentences and expressing that understanding numerically.

In fact, if we look a little deeper into these clusters of numbers—developers call them ***embedding vectors***—truly strange things happen. Will similar sentences produce similar clusters of numbers created by BERT? That's obvious.

We can even do things like this: For example,
- "I eat rice."
- "The meeting earlier was fun."
- "Who comes to drink from the spring in the deep mountains?"
- "Going to Mars exploration is too futuristic."

If we convert all of these into clusters of numbers—developers call this ***embedding***—and then convert the question "**What ideas are beneficial to Earth?**" into a cluster of numbers, and rank it by proximity to the four clusters created earlier? Surprisingly, **"Going to Mars exploration is too futuristic."** comes out on top.

<blockquote>
**Unlike ChatGPT, BERT could not speak. So, they expressed their intentions through numbers.**
</blockquote>


## The Meeting of BERT and GPT

<CaptionCenteredImage 
  imageSrc={colaborateImage}
  caption="Imagining a system where two different AIs collaborate was very natural."
  imageAlt="Imagining a system where two different AIs collaborate was very natural."
/>

<blockquote>
**"But BERT could not speak."**
</blockquote>

We have met two geniuses so far. One was a fluent **Writer (GPT)**, the other a silent **Librarian (BERT)**. The **Writer** could write magnificent prose but had a terrible memory. The **Librarian**, on the other hand, was well-versed in a vast sea of knowledge but could not express its thoughts aloud.

<blockquote>
**"What if we combined these two?"**
</blockquote>

This question was the starting point for a technology called **Retrieval-Augmented Generation**, or **RAG**.<sup><a href='#cite-ref-4' class='inline-link'>4</a></sup> This is like taking an **open-book exam**.

### The Open-Book Exam, RAG

1.  **Question:** The user asks **GPT**, "Explain the balance of acidity and sweetness in specialty coffee."
2.  **Search Request:** Before answering, **GPT** tells **BERT** the **core keywords** of the question (e.g., "specialty coffee," "acidity," "sweetness," "balance").
3.  **Knowledge Provision:** **BERT** searches its vast library of number clusters and finds **a single book containing the most relevant information**—meaningfully closest to the keywords—and hands it to **GPT**.
4.  **Generation:** **GPT** uses that page as a reference and eloquently composes an answer, as if it had known the answer all along.

<blockquote>
**"RAG is, ultimately, a technology that conveys the wisdom of the mute genius librarian (BERT) to the world through the mouth of the eloquent writer (GPT)."**
</blockquote>

This was one of the most ingenious solutions in AI history. It was like assigning a personal librarian to an AI with no memory, to find the exact references it needs, whenever it needs them.


## Much text has passed. How has your first impression of me and us changed now?

<CaptionCenteredImage 
  imageSrc={coffeeAndTimeImage}
  caption="What emotions come to mind when you see this photo again?"
  imageAlt="What emotions come to mind when you see this photo again?"
  className='h-[500px] object-cover'
/>

One evening, I lean back in my chair and drink coffee late at night. Along with the bitter taste, a subtle, melancholy aroma rises. I often feel this way when working late into the night. This melancholic feeling and bitter coffee actually have entirely unrelated meanings. But in my mind, through habitual actions, they have somehow come to form a single semantic whole, sharing the same fate.

Now, to ChatGPT, with whom many conversation records have accumulated, I asked again about coffee: **I am drinking coffee now.**

-   *ChatGPT performs RAG by indexing previous conversation records through BERT.*
-   *It retrieves all records closely related to coffee semantically.*
-   *It pulls together every coffee ever consumed before.*

And then it replies: **Which coffee—Ethiopian, Kenyan, or Brazilian—that you've had before does this taste similar to? I'm curious.**

Just then, my loving mother opens the door and enters.

<blockquote>
**"Stop working on the computer today. Why are you drinking coffee at night? It'll ruin your health."**
</blockquote>

## vBERT, the Alchemist of Time: Breaking Free from the Prison of Meaning to Understand Time

ChatGPT replied very politely and excellently. Referring to the 'library of meaning' that is RAG, it perfectly retrieved and 'semantically' connected all past conversation records related to coffee. But it utterly failed to grasp the **'temporal context'**—so crucial to us—of the night, the coffee, and the worry of solitude and family.

To RAG, my mother's story is just 'another text fragment.' It doesn't know that the connection between a mother's daily worry and drinking coffee at night is another monumental 'event.'

<blockquote>
**"What if AI were shown two separate books (the coffee and mother's story) in chronological order, and then asked to summarize what happened in that 'flow of time' into a single vector?"**
</blockquote>

This was the core question of **vBERT (Vectorized BERT)**. Unlike BERT, which learns sequences of words (Text), vBERT was born to learn the **'sequence of meaning chunks (Vectors), i.e., Sequence of Embeddings'** itself.

<CaptionCenteredImage 
  imageSrc={filmMovieImage}
  caption="AI reading the flow of meaning like a film"
  imageAlt=""
  className='h-[500px] object-cover'
/>

vBERT does not view the 'bitter coffee' vector, 'lonely feeling' vector, and 'mother's worry' vector, placed in the flow of time, as independent entities. Instead, vBERT synthesizes these three separate meaning chunks (vectors) within a 'temporal flow,' ultimately condensing them into a single new vector representing the **'atmosphere of that night.'**

Just as individual scenes (vectors) in a film combine to create the overarching emotion (a single new vector) of the entire movie, vBERT is the **'Alchemist of Time,'** refining multiple temporally ordered embedding sequences into a single final 'sequence embedding.'

<blockquote>
**"vBERT enables AI to finally break free from the prison of 'meaning' and to understand 'time' in a new era."**
</blockquote>

Now, AI can move beyond merely predicting the next word or identifying the most relevant document. It can grasp how events unfold in sequence, forming the complete tapestry of a story's context.

<br></br>

<br></br>

# The Sequence Merger : Who links the books into the single narrative

<br></br>

<CaptionCenteredImage 
  imageSrc={oldBooksImage}
  caption="Each of the book has its own flow. However, The streamline made from the books has its own vibe."
  imageAlt="Each of the book has its own flow. However, The streamline made from the books has its own vibe."
/>


RAG is an expert librarian, capable of instantly finding the single most relevant book from a vast collection. But what if that 'book' is actually just one volume of a sprawling epic novel? RAG, focused on immediate relevance, might hand you Volume 5 first, oblivious to the grand narrative that unfolds only by reading Volume 1 through to Volume 4.

Our Sequence Merger is different. It is the first 'reader' that understands *why* an epic story is divided into multiple volumes. It doesn't just extract information; it comprehends the flow, the development, the temporal coherence that weaves individual 'books' (embedding vectors) into a continuous, evolving 'narrative' (a single, new sequence embedding). It transforms a series of discrete vectors into a cohesive story, much like a seasoned reader experiences a saga from beginning to end.

## Core Philosophy: The Extrapolation Hypothesis

<CaptionCenteredImage 
  imageSrc={childImage}
  caption="Could the 512-token child can grow then predict over its teacher's limit?"
  imageAlt="Could the 512-token child can grow then predict over its teacher's limit?"
/>


At the core of the Sequence Merger lies the **Extrapolation Hypothesis**—our guiding principle for transcending the inherent context length limitations of base embedding models like `intfloat/multilingual-e5-base` (max 512 tokens).

We posit that by training a lightweight model to synthesize a single representative embedding from N individual chunk embeddings—where each chunk and their concatenation fit within the base model's context limit—the model learns not mere memorization, but the fundamental *function* of semantic synthesis. This function captures the relational dynamics and sequential coherence among vectors, allowing the model to preserve and integrate meaning progressively.

### From N-to-1 Synthesis to Infinite Extrapolation
- **Training Phase (Within Limits):** During training, inputs are N chunk embeddings `[vec(A), vec(B), ..., vec(N)]`, derived from texts where the full concatenation `A + B + ... + N` remains ≤512 tokens. The target is the base model's direct embedding `vec(A + B + ... + N)`, ensuring ground-truth supervision. This teaches the model to merge sequences reliably within bounded contexts.

- **Inference Phase (Beyond Limits):** In deployment, the model's output—itself a synthesized vector—becomes an input for the next merge. By recursively chaining outputs (e.g., merge `[vec(A)..vec(K)]` to synth_vec(A..K), then merge with `[vec(K+1)..vec(M)]`), the model extrapolates to arbitrarily long sequences, far exceeding 512 tokens. This self-referential synthesis maintains 'semantic momentum,' avoiding catastrophic loss of earlier context.

### Key Implications
- **Extended Context:** Enables processing of documents, conversations, or streams of any length as a single coherent embedding, without relying on resource-intensive long-context models.
- **Efficiency and Autonomy:** No need for repeated API calls or full-sequence re-embedding; operates on pre-computed chunks, reducing latency and cost to near-zero beyond initial embedding.
- **Validation:** Initial experiments show stable cosine similarity (>0.75) in chained merges up to 10x the base limit, with minimal degradation in recall of initial chunks—proving the hypothesis's viability.

## Key Configurable Parameters

Our architecture shines through its flexible hyperparameters, allowing fine-tuned control over the pipeline:

- **d_model**: Embedding dimension (default: 768, E5-base compatible for seamless integration).
- **num_layers**: Depth of the transformer stack (e.g., 4-7 layers for balanced performance).
- **nhead**: Number of attention heads (default: 8; per-layer variations supported).
- **layer_dims**: Variable intermediate dimensions per layer (e.g., `[3840, 1920, 768]` for progressive compression—our signature bottleneck design).
- **ffn_dims**: Custom feed-forward hidden sizes per layer (e.g., `[8192, 6144, 4096]` for GLU-enhanced expansion).
- **nhead_dims**: Per-layer head counts (e.g., `[4, 3, 2, 2, 2]` for adaptive attention scaling).
- **use_glu**: Enable Gated Linear Units in FFN for non-linear gating (default: False; boosts expressivity in complex merges).
- **dropout**: Regularization rate (e.g., 0.1-0.12 for stable training).

These parameters enable architectures with 137M to 206M parameters, optimized for efficiency and coherence. Explore variants by tweaking `config.json` and retraining!

## Training Philosophy: The Principle of Progressive Overload Fine-Tuning

Our training paradigm emerges from a profound failure-turned-insight: an experiment with extreme datasets that initially devastated performance but ultimately revealed the path to superior generalization.

### The Extreme Dataset: 512-to-1 and Its Catastrophic Debut
To maximize data efficiency, we engineered an 'extreme' dataset by chunking source articles into single-token units—the atomic building blocks of embeddings. This created 512-to-1 pairs: inputs of 512 individual token embeddings, targeting the base model's direct embedding of the full sequence. From just hundreds of original articles, this yielded tens of thousands of input-target pairs, far outstripping the standard 10-to-1 dataset (merging up to 10 ~50-token chunks).

However, directly training on this 512-to-1 data was nothing short of catastrophic. Our models didn't just perform poorly; they suffered **catastrophic forgetting**, with benchmark scores plummeting into oblivion. The sheer, overwhelming complexity of atomic extremes shattered the fragile structure of our lightweight mergers. It was a brutal lesson: brute-force exposure to the most granular data destroyed, rather than built, capability. We faced a profound failure, a moment where all seemed lost.

### The Breakthrough: Progressive Overload as Pre-Training + Fine-Tuning – Embracing Failure as Our Forge
The pivot came from reframing this devastating failure not as an end, but as a crucible. Instead of despairing, we envisioned a two-stage 'Progressive Overload' approach, directly inspired by athletic training: foundational strength must precede maximum load. This was our 'eureka' moment, a desperate gamble that yielded unprecedented results.

1. **Stage 1: Atomic Pre-Training (512-to-1 as Foundation):** Feed the model 512-to-1 data first, with higher learning rates. This 'searing' phase intentionally disrupts and rebuilds the model's internals, teaching it the primal interactions of token-level vectors—like forging atomic bonds in a vacuum. Though scores crash initially, it instills a robust 'semantic substrate' for handling fine-grained relationships.

2. **Stage 2: Molecular Fine-Tuning (10-to-1 as Specialization):** Transition to the target 10-to-1 dataset. This 'refinement' phase leverages the pre-trained foundation to master higher-level synthesis, chaining atomic insights into coherent 'molecular' embeddings. Remarkably, a single epoch yields dramatic recovery, surpassing vanilla 10-to-1 training.

By embracing failure as the crucible for growth, Progressive Overload ensures our mergers evolve from fragile aggregators to resilient synthesizers—ready for the infinite extrapolation promised by our core philosophy.

### Key Innovation: Paired Positional Encoding (PPE)
Within this paradigm, PPE addresses the challenge of injecting sequence order into high-dimensional chunk embeddings without distorting their rich semantics. Traditional positional encodings add noise-like signals, degrading benchmarks; PPE duplicates the sequence and *overwrites* dedicated dimension slots (front/back split) with position vectors, preserving 90%+ of original meaning while enabling precise positional awareness.

## Core Self-Supervised Method: vBERT

vBERT represents our crowning achievement: a vector-native adaptation of BERT-style pre-training, tailored for sequence mergers. By leveraging PPE, vBERT uses two intertwined self-supervised tasks—vMLM (violated Masked Language Model) and vNSP (violated Next Sentence Prediction)—to instill temporal and semantic coherence in the model. These tasks run concurrently, with total loss = loss_vmlm + loss_vnsp, enabling the model to evolve from a mere aggregator to a deep synthesizer of meaning.

### Critical Prerequisite: The `1-Article-1-Sequence` Dataset

vBERT's self-supervised pre-training requires a meticulously curated dataset to preserve the integrity of its learning signals, especially for vNSP's continuity detection.

Standard n-to-1 datasets (e.g., 512-to-1 or 10-to-1) are incompatible for vBERT:

- **Self-Supervised Nature:** vBERT does not rely on ground-truth to-1 vectors; attempts to use such datasets introduce unnecessary supervisory noise absent in pure self-supervision.
- **The False History Paradox:** Sliding-window construction generates overlapping, semantically coherent sequences from the same source. In vNSP, creating 'false histories' by splicing can inadvertently produce near-identical continuations (e.g., adjacent overlaps), leading to false negatives. The model learns to flag coherent narratives as 'disrupted,' creating a schizophrenic training signal that erodes context awareness and caps pre-training efficacy.

To eliminate this, we developed a dedicated pre-training corpus from `wikipedia/wikipedia` (multi-language editions like en, ko, fr, etc.) 1 Article = 1 Sequence. Process each full article into a single, unbroken vector sequence without sliding windows or partial excerpts.

### vMLM: Self-Consistent Vector Reconstruction
vMLM refines the MLM paradigm for vectors, enforcing that predictions maintain global sequence coherence via a feedback injection loop.

1. **Masking:** Randomly mask 15% of tokens in the input sequence, e.g., [A, B, C, D, E] → [A, M, C, D, E].
2. **Forward Pass on Masked Sequence:** Pass through the model with PPE duplication, yielding hidden layer outputs [cls', a1', m1', c1', d1', e1', a2', m2', c2', d2', e2']. Extract the predicted masked vectors m1' (first duplicate) and m2' (second duplicate).
3. **Original CLS Acquisition:** Pass the unmasked original sequence through the model to obtain CLS_orig (the final CLS token after PPE processing).
4. **Prediction Injection:** Re-pass the masked sequence, but during PPE, stealthily replace the masked positions with the predicted m1' and m2' (integrating into the appropriate front/back slots). This generates CLS_guesses (new CLS token).
5. **Strict L1 Loss Computation:** Calculate L1 loss (Mean Absolute Error across all 768 dimensions) between CLS_guesses and CLS_orig. Unlike lenient cosine similarity, L1 demands exact matching of both direction *and* magnitude, forging unyielding precision in reconstructions.

### vNSP: Binary Continuity Classification
vNSP upgrades NSP for vectors, using a classification head to detect sequence authenticity and penalize disruptions.

1. **History Generation:** For 50% of batches, create false histories: split the sequence at mid-point (split_idx = len // 2), discard the latter half, and splice in a random prefix from another sequence (for this reason, database should not be neither the 512-to-1 nor the 10-to-1. this will be discuss later.), e.g., [A, B, C] → [A, B, D'] where D' disrupts flow.
2. **PPE Processing:** Apply PPE to true/false sequences independently, embedding positional cues to highlight discontinuities in false cases.
3. **Classification Head:** Forward to get normalized CLS (cls_norm), pass through a dedicated NSP head (nn.Linear(768, 2)) yielding logits [IsNext, NotNext].
4. **Cross-Entropy Loss:** Label true as 0 (IsNext), false as 1 (NotNext); compute CE loss. High ppe_dim amplifies temporal breaks; low ppe_dim focuses on semantic jarring.

## The 3-Stage Rocket Protocol: Unified Training Philosophy

At the apex of our Extrapolation Hypothesis lies the 3-Stage Rocket Protocol—a comprehensive, battle-tested blueprint that propels models from naive aggregators to extrapolative synthesizers. Forged from iterative experimentation, this protocol synthesizes our key innovations (PPE, vBERT) and strategies (Progressive Overload) into a seamless ascent: Stage 1 builds philosophical foundations, Stage 2 stress-tests through overload, and Stage 3 refines for peak performance. vBERT, though disastrous in isolation (benchmarks plummeting as loss decreases), shines as the irreplaceable 'launchpad'—pre-pre-training that awakens innate vector synthesis principles.

#### Stage 1: Pre-Pre-Training – vBERT (Instilling First Principles)

vBERT serves as the 'temporal archaeologist' phase: a self-supervised pre-training regime that etches core principles of vector coherence into a blank-slate model, using PPE (high ppe_dim=384) to inject positional fidelity without semantic distortion.

**Key Enabler: Paired Positional Encoding (PPE)** 
PPE duplicates sequences and overwrites dedicated slots (front/back m/2 splits, m=ppe_dim) with positional vectors, preserving 90%+ original meaning. High ppe_dim (384) prioritizes temporal stability, preventing gradient explosions in self-supervision. 

#### Stage 2: High-Difficulty Fine-Tuning – Progressive Overload (512-to-1 Awakening)

With vBERT's foundations laid, Stage 2 applies the 'intentional destruction' of Progressive Overload: expose the primed model to atomic-level extremes via 512-to-1 datasets (token-chunked, all combination trees). **Low ppe_dim=16** shifts to 'meaning insightful' mode, minimizing temporal distortion.

#### Stage 3: Standard Fine-Tuning – Peak Refinement (10-to-1 Mastery)

Stage 3 polishes the awakened model with 10-to-1 datasets (50-token chunks, up to 10 elements), **maintaining ppe_dim=16** for semantic dominance. 

## Benchmark Philosophy: Robust Separation Score (RSS) for RAG Utility

Our Extrapolation Hypothesis demands not just theoretical elegance but empirical proof in real-world utility: Retrieval-Augmented Generation (RAG) systems. Traditional coherence metrics isolate internal consistency; we prioritize **RAG robustness**—ensuring synthesized vectors act as clear signals for relevant memories while drowning irrelevant noise, preventing hallucinations and off-topic retrievals.

### Critical Prerequisite: Semantically Distinct Probes

For benchmarks to measure true synthesis fidelity rather than data artifacts, all test chunks (A, B, C, ..., N) must exhibit **semantic independence**: no pre-existing thematic, stylistic, or topical similarities that could inflate or confound cosine scores. If, e.g., A and C share content from the same domain, synth(ABC) similarity to embed(C) might reflect correlation, not model merit—rendering evaluations invalid.

Mirroring the vBERT pre-training corpus curation:

- **Dataset Source:** Multi-language Wikipedia (wikipedia/wikipedia) editions (e.g., en, ko, fr, de, ja—spanning histories, sciences, arts, etc.).
- **Generation Principle:** Shuffle articles globally, then apply '1 Article = 1 Probe' strictly: each full article processes into one unique chunk/embedding (embed(A)), without sliding windows, excerpts, or intra-article splits.
- **Key Benefits:** Ensures probes originate from disparate 'worlds'—e.g., A from a Korean history article, B from French philosophy—guaranteeing baseline dissimilarities (avg. cosine \<0.1 pre-synthesis). This purity isolates model-induced affinities in RSS gaps, validating RAG signal strength.
- **Implementation:** Custom pipeline similar to `data_pipeline.py` (10k-50k probes per language mix; no Devil randomization to preserve test integrity). Impure probes (e.g., same-language clusters) cap scores at \<5, subverting our 10+ FINESSE triumphs.

This prerequisite is foundational; benchmarks without it risk false positives, undermining the Extrapolation Hypothesis.

### Core Setup
Given a document sequence of chunks A, B, C, D, E with pre-computed embeddings embed(A), ..., embed(E), our model generates cumulative syntheses: synth(AB) from embed(A)+embed(B), synth(ABC) from synth(AB)+embed(C), up to synth(ABCDE). Evaluations probe bidirectional fidelity: how well syntheses 'recognize' their constituents (Top-Down) and how constituents 'recognize' containing syntheses (Bottom-Up).

### Top-Down Evaluation: Synthesis-to-Constituents Fidelity
Tests if a synthesis preserves affinity to its building blocks while rejecting outsiders. E.g., synth(ABC) should exhibit high cosine similarity to embed(A), embed(B), embed(C) (Tier 1: Memory Group X) but low to embed(D), embed(E) (Tier 2: Noise Group Y). This validates 'root retention': the synthesis hasn't forgotten its origins amid merging.

### Bottom-Up Evaluation: Constituents-to-Syntheses Affinity
Complements Top-Down by inverting perspective: a constituent should affinity-match containing syntheses but reject non-containing ones. E.g., embed(C) low similarity to synth(AB) (Noise Y) but high to synth(ABC), synth(ABCD), synth(ABCDE) (Memory X). This ensures 'role persistence': chunks remain anchored in their composite contexts.

### Robust Separation Score (RSS): Quantifying the Gap
For any validator p (synthesis or constituent) against Groups X (expected similar) and Y (expected dissimilar):

1. Compute all pairwise cosines: sims_X = [cos(p, x) for x in X], sims_Y = [cos(p, y) for y in Y].
2. Sort sims_X ascending, sims_Y ascending.
3. Gap = Q1_X (25th percentile of sims_X, weakest expected matches) - Q3_Y (75th percentile of sims_Y, strongest noise).
4. Normalize gap to [0,1] score via min-max scaling (accounting for cosine [-1,1] bounds).

RSS demands the 'barely similar' (Q1_X) decisively outperform the 'most tempting noise' (Q3_Y), creating a 'silent margin' for RAG resilience. Quartiles ensure outlier-robustness: no single perfect match skews results.

### Final FINESSE Scoring: Balance with Penalty
Aggregate td (Top-Down RSS average) and bu (Bottom-Up RSS average) as [0,1] scores:

final = [ (td + bu)/2 - |td - bu| ] × 500

- **Average:** Rewards overall separation.
- **Imbalance Penalty (|td - bu|):** Discourages lopsided specialists; true mastery requires bidirectional consistency.
- **Scaling (×500):** Transforms to intuitive [-1000, +1000] range.

## Acknowledgments

We extend our profoundest thanks to the **intfloat team** and the creators of the `multilingual-e5-base` model. This groundbreaking embedding model was the very foundation of our project: all training datasets were meticulously generated using E5 embeddings, our evaluations were judged against E5 as the gold standard benchmark, and the Synthesizer architecture was specifically designed in symbiotic harmony with E5's multilingual capabilities—making it an organic extension rather than a standalone entity. Without their visionary work in advancing multilingual representation, the Tiny Sequence Merger simply would not exist. Their open-source contribution is the true seed from which our innovations grew.

Built with PyTorch and Transformers. For more on the underlying research, check our project logs.

# vBERT: Unlocking the Timeless Context

## Overview: The AI That Remembers Beyond Words

Even in the richest conversations, the nuances of our beginnings often fade. Imagine savoring a cherished coffee with a friend—the initial burst of flavor, the comforting aroma—only for it to abstract into 'just a good coffee' over time. We strive to retain context, yet each broken link in our memory's chain risks unraveling the whole. Does the subtle memory of coffee's taste truly just vanish?

No. Humans possess a unique power untouched by current AI: the ability to beautifully forget, yet flexibly remember. We compress and faintly connect memories. While the impression of coffee's nuanced aroma might fade, a similar scent can instantly conjure thoughts of old friends and shared moments. This is the flexibility of human memory.

### The AI That Forgets: The Transformer Decoder, BERT, and RAG's Limitations

For nearly a decade, the Transformer architecture, particularly its decoder variant (the engine behind models like ChatGPT), revolutionized how AI processes language. These models became masters of generating fluent text, akin to a continuous storytelling game. However, much like in a relay story, their 'context window' imposes a hard limit. Information beyond this window simply vanishes, disregarded as if a page ripped from a book.

Then came BERT, the 'silent librarian.' A master of understanding, BERT could analyze an entire sentence, capturing its essence in a numerical 'embedding vector'—a distilled wisdom of its meaning, nuance, and emotion. This led to Retrieval-Augmented Generation (RAG), a brilliant symbiotic approach: the articulate writer (GPT) could consult the silent librarian (BERT) for relevant information, making AI appear to have a memory.

RAG offers profound advancements. Yet, an ultimate question remains: Does it truly understand *time*?

Consider this scenario, often encountered: You're deep into a night of work, the bitter taste of coffee mingling with a subtle, melancholic aroma—a feeling of solitude. You ask ChatGPT about your coffee. The model, consulting its RAG-powered 'library,' recalls every coffee-related discussion: "Ethiopian? Kenyan? Brazilian?" it enthusiastically asks. Then, a loved one enters, perhaps a mother, saying: "Stop working on the computer. Coffee late at night isn't good for you. You'll ruin your health."

ChatGPT offers an excellent, polite reply, drawing from its 'library of meaning' to perfectly connect with past coffee conversations. But it utterly fails to grasp the **'temporal context'** of the mother's entrance—an event semantically unrelated to coffee, yet profoundly significant to the human experience. To RAG, the mother's worried words are just another text snippet; it cannot recognize this as a monumental event redefining the entire conversation's meaning within that specific moment.

This is where vBERT enters.

vBERT is a custom neural architecture designed for **intelligent merging of variable-length vector sequences into a single representative vector**. It excels at tasks requiring sequence summarization, such as embedding aggregation in NLP or multimodal fusion, by capturing nuanced relational dynamics that extend beyond mere semantic similarity to encompass temporal coherence.

This model transforms an input of N arbitrary vectors `(batch_size, seq_len, d_model)` into a fixed output `(batch_size, d_model)`, preserving semantic coherence while adapting to complex data flows.
Please note that we trained this model fitting into the  **intfloat/mutilingual-e5-base** embed. Although the learning & training methology could be applied into other embedding models, this model follows the [intfloat/mutilingual-e5-base](https://huggingface.co/intfloat/multilingual-e5-base) as the elemental embedding engine.

## Scale

[[[please fill this section with the proper database & model scale.]]]

The database has limited n=10, so this model **Might not properly handle mode than 10 seq of vectors**. 


## Core Philosophy: The Extrapolation Hypothesis

At the core of the Sequence Merger lies the **Extrapolation Hypothesis**—our guiding principle for transcending the inherent context length limitations of base embedding models like `intfloat/multilingual-e5-base` (max 512 tokens).

### The Hypothesis
We posit that by training a lightweight model to synthesize a single representative embedding from N individual chunk embeddings—where each chunk and their concatenation fit within the base model's context limit—the model learns not mere memorization, but the fundamental *function* of semantic synthesis. This function captures the relational dynamics and sequential coherence among vectors, allowing the model to preserve and integrate meaning progressively.

### From N-to-1 Synthesis to Infinite Extrapolation
- **Training Phase (Within Limits):** During training, inputs are N chunk embeddings `[vec(A), vec(B), ..., vec(N)]`, derived from texts where the full concatenation `A + B + ... + N` remains ≤512 tokens. The target is the base model's direct embedding `vec(A + B + ... + N)`, ensuring ground-truth supervision. This teaches the model to merge sequences reliably within bounded contexts.

- **Inference Phase (Beyond Limits):** In deployment, the model's output—itself a synthesized vector—becomes an input for the next merge. By recursively chaining outputs (e.g., merge `[vec(A)..vec(K)]` to synth_vec(A..K), then merge with `[vec(K+1)..vec(M)]`), the model extrapolates to arbitrarily long sequences, far exceeding 512 tokens. This self-referential synthesis maintains 'semantic momentum,' avoiding catastrophic loss of earlier context.

### Key Implications
- **Extended Context:** Enables processing of documents, conversations, or streams of any length as a single coherent embedding, without relying on resource-intensive long-context models.
- **Efficiency and Autonomy:** No need for repeated API calls or full-sequence re-embedding; operates on pre-computed chunks, reducing latency and cost to near-zero beyond initial embedding.
- **Validation:** Initial experiments show stable cosine similarity (>0.75) in chained merges up to 10x the base limit, with minimal degradation in recall of initial chunks—proving the hypothesis's viability.

## Usage

The TransformerSynthesizer processes **pre-computed vector sequences** (e.g., embeddings from E5), not raw text. Load and use it via the Hugging Face Hub with `trust_remote_code=True`. Below is a realistic workflow integrating with `intfloat/multilingual-e5-base` for text-to-vector conversion.

```python
from transformers import AutoTokenizer, AutoModel
import torch

# Step 1: Load E5 tokenizer and model for embedding generation
tokenizer = AutoTokenizer.from_pretrained('intfloat/multilingual-e5-base')
e5_model = AutoModel.from_pretrained('intfloat/multilingual-e5-base')

# Example: Batch of texts to merge (e.g., multiple sentences per document)
texts = [
    ["First sentence of batch 1.", "Second sentence of batch 1.", "Third sentence of batch 1."],
    ["First sentence of batch 2.", "Second sentence of batch 2."]  # Variable lengths
]
batch_size = len(texts)

# Step 2: Convert texts to embeddings using E5 (batch_size, seq_len, d_model)
input_sequences = []
for i in range(batch_size):
    inputs = tokenizer(texts[i], return_tensors='pt', padding=True, truncation=True)
    with torch.no_grad():
        e5_outputs = e5_model(**inputs)
        # Mean pool E5 outputs to get sentence embeddings (seq_len, d_model)
        sentence_embeddings = e5_outputs.last_hidden_state.mean(dim=1)
    input_sequences.append(sentence_embeddings)

input_sequences = torch.cat(input_sequences, dim=0)  # (batch_size, seq_len, 768)

# Step 3: Load our Synthesizer model
synthesizer = AutoModel.from_pretrained(
    "your-username/tiny-sequence-merger-soft-large",
    trust_remote_code=True,
    torch_dtype=torch.float16  # Optional: for efficiency
)

# Step 4: Forward pass to merge sequences
with torch.no_grad():
    merged_vectors = synthesizer(input_sequences).last_hidden_state  # Shape: (batch_size, d_model)

print(f"Merged vectors shape: {merged_vectors.shape}")
print("Synthesized embeddings ready for downstream tasks!")
```

This workflow highlights our model's role as a 'vector synthesizer': it takes E5 embeddings as input and produces a coherent, single representation per sequence. For configuration details, inspect `config.json`. The model supports batched, variable-length inference on GPU/CPU and integrates with the Transformers pipeline for downstream tasks.

## Key Configurable Parameters

Our architecture shines through its flexible hyperparameters, allowing fine-tuned control over the pipeline:

- **d_model**: Embedding dimension (default: 768, E5-base compatible for seamless integration).
- **num_layers**: Depth of the transformer stack (e.g., 4-7 layers for balanced performance).
- **nhead**: Number of attention heads (default: 8; per-layer variations supported).
- **layer_dims**: Variable intermediate dimensions per layer (e.g., `[3840, 1920, 768]` for progressive compression—our signature bottleneck design).
- **ffn_dims**: Custom feed-forward hidden sizes per layer (e.g., `[8192, 6144, 4096]` for GLU-enhanced expansion).
- **nhead_dims**: Per-layer head counts (e.g., `[4, 3, 2, 2, 2]` for adaptive attention scaling).
- **use_glu**: Enable Gated Linear Units in FFN for non-linear gating (default: False; boosts expressivity in complex merges).
- **dropout**: Regularization rate (e.g., 0.1-0.12 for stable training).

These parameters enable architectures with 137M to 206M parameters, optimized for efficiency and coherence. Explore variants by tweaking `config.json` and retraining!

## Training Philosophy: The Principle of Progressive Overload Fine-Tuning

Our training paradigm emerges from a profound failure-turned-insight: an experiment with extreme datasets that initially devastated performance but ultimately revealed the path to superior generalization.

### The Extreme Dataset: 512-to-1 and Its Catastrophic Debut
To maximize data efficiency, we engineered an 'extreme' dataset by chunking source articles into single-token units—the atomic building blocks of embeddings. This created 512-to-1 pairs: inputs of 512 individual token embeddings, targeting the base model's direct embedding of the full sequence. From just hundreds of original articles, this yielded tens of thousands of input-target pairs, far outstripping the standard 10-to-1 dataset (merging up to 10 ~50-token chunks).

However, directly training on this 512-to-1 data was nothing short of catastrophic. Our models didn't just perform poorly; they suffered **catastrophic forgetting**, with benchmark scores plummeting into oblivion. The sheer, overwhelming complexity of atomic extremes shattered the fragile structure of our lightweight mergers. It was a brutal lesson: brute-force exposure to the most granular data destroyed, rather than built, capability. We faced a profound failure, a moment where all seemed lost.

### The Breakthrough: Progressive Overload as Pre-Training + Fine-Tuning – Embracing Failure as Our Forge
The pivot came from reframing this devastating failure not as an end, but as a crucible. Instead of despairing, we envisioned a two-stage 'Progressive Overload' approach, directly inspired by athletic training: foundational strength must precede maximum load. This was our 'eureka' moment, a desperate gamble that yielded unprecedented results.

1. **Stage 1: Atomic Pre-Training (512-to-1 as Foundation):** Feed the model 512-to-1 data first, with higher learning rates. This 'searing' phase intentionally disrupts and rebuilds the model's internals, teaching it the primal interactions of token-level vectors—like forging atomic bonds in a vacuum. Though scores crash initially, it instills a robust 'semantic substrate' for handling fine-grained relationships.

2. **Stage 2: Molecular Fine-Tuning (10-to-1 as Specialization):** Transition to the target 10-to-1 dataset. This 'refinement' phase leverages the pre-trained foundation to master higher-level synthesis, chaining atomic insights into coherent 'molecular' embeddings. Remarkably, a single epoch yields dramatic recovery, surpassing vanilla 10-to-1 training.

By embracing failure as the crucible for growth, Progressive Overload ensures our mergers evolve from fragile aggregators to resilient synthesizers—ready for the infinite extrapolation promised by our core philosophy.

### Key Innovation: Paired Positional Encoding (PPE)
Within this paradigm, PPE addresses the challenge of injecting sequence order into high-dimensional chunk embeddings without distorting their rich semantics. Traditional positional encodings add noise-like signals, degrading benchmarks; PPE duplicates the sequence and *overwrites* dedicated dimension slots (front/back split) with position vectors, preserving 90%+ of original meaning while enabling precise positional awareness.

## Core Self-Supervised Method: vBERT

vBERT represents our crowning achievement: a vector-native adaptation of BERT-style pre-training, tailored for sequence mergers. By leveraging PPE, vBERT uses two intertwined self-supervised tasks—vMLM (violated Masked Language Model) and vNSP (violated Next Sentence Prediction)—to instill temporal and semantic coherence in the model. These tasks run concurrently, with total loss = loss_vmlm + loss_vnsp, enabling the model to evolve from a mere aggregator to a deep synthesizer of meaning.

### Critical Prerequisite: The `1-Article-1-Sequence` Dataset

vBERT's self-supervised pre-training requires a meticulously curated dataset to preserve the integrity of its learning signals, especially for vNSP's continuity detection.

Standard n-to-1 datasets (e.g., 512-to-1 or 10-to-1) are incompatible for vBERT:

- **Self-Supervised Nature:** vBERT does not rely on ground-truth to-1 vectors; attempts to use such datasets introduce unnecessary supervisory noise absent in pure self-supervision.
- **The False History Paradox:** Sliding-window construction generates overlapping, semantically coherent sequences from the same source. In vNSP, creating 'false histories' by splicing can inadvertently produce near-identical continuations (e.g., adjacent overlaps), leading to false negatives. The model learns to flag coherent narratives as 'disrupted,' creating a schizophrenic training signal that erodes context awareness and caps pre-training efficacy.

To eliminate this, we developed a dedicated pre-training corpus from `wikipedia/wikipedia` (multi-language editions like en, ko, fr, etc.) 1 Article = 1 Sequence. Process each full article into a single, unbroken vector sequence without sliding windows or partial excerpts.

### vMLM: Self-Consistent Vector Reconstruction
vMLM refines the MLM paradigm for vectors, enforcing that predictions maintain global sequence coherence via a feedback injection loop.

1. **Masking:** Randomly mask 15% of tokens in the input sequence, e.g., [A, B, C, D, E] → [A, M, C, D, E].
2. **Forward Pass on Masked Sequence:** Pass through the model with PPE duplication, yielding hidden layer outputs [cls', a1', m1', c1', d1', e1', a2', m2', c2', d2', e2']. Extract the predicted masked vectors m1' (first duplicate) and m2' (second duplicate).
3. **Original CLS Acquisition:** Pass the unmasked original sequence through the model to obtain CLS_orig (the final CLS token after PPE processing).
4. **Prediction Injection:** Re-pass the masked sequence, but during PPE, stealthily replace the masked positions with the predicted m1' and m2' (integrating into the appropriate front/back slots). This generates CLS_guesses (new CLS token).
5. **Strict L1 Loss Computation:** Calculate L1 loss (Mean Absolute Error across all 768 dimensions) between CLS_guesses and CLS_orig. Unlike lenient cosine similarity, L1 demands exact matching of both direction *and* magnitude, forging unyielding precision in reconstructions.

### vNSP: Binary Continuity Classification
vNSP upgrades NSP for vectors, using a classification head to detect sequence authenticity and penalize disruptions.

1. **History Generation:** For 50% of batches, create false histories: split the sequence at mid-point (split_idx = len // 2), discard the latter half, and splice in a random prefix from another sequence (for this reason, database should not be neither the 512-to-1 nor the 10-to-1. this will be discuss later.), e.g., [A, B, C] → [A, B, D'] where D' disrupts flow.
2. **PPE Processing:** Apply PPE to true/false sequences independently, embedding positional cues to highlight discontinuities in false cases.
3. **Classification Head:** Forward to get normalized CLS (cls_norm), pass through a dedicated NSP head (nn.Linear(768, 2)) yielding logits [IsNext, NotNext].
4. **Cross-Entropy Loss:** Label true as 0 (IsNext), false as 1 (NotNext); compute CE loss. High ppe_dim amplifies temporal breaks; low ppe_dim focuses on semantic jarring.

## The 3-Stage Rocket Protocol: Unified Training Philosophy

At the apex of our Extrapolation Hypothesis lies the 3-Stage Rocket Protocol—a comprehensive, battle-tested blueprint that propels models from naive aggregators to extrapolative synthesizers. Forged from iterative experimentation, this protocol synthesizes our key innovations (PPE, vBERT) and strategies (Progressive Overload) into a seamless ascent: Stage 1 builds philosophical foundations, Stage 2 stress-tests through overload, and Stage 3 refines for peak performance. vBERT, though disastrous in isolation (benchmarks plummeting as loss decreases), shines as the irreplaceable 'launchpad'—pre-pre-training that awakens innate vector synthesis principles.

#### Stage 1: Pre-Pre-Training – vBERT (Instilling First Principles)

vBERT serves as the 'temporal archaeologist' phase: a self-supervised pre-training regime that etches core principles of vector coherence into a blank-slate model, using PPE (high ppe_dim=384) to inject positional fidelity without semantic distortion.

**Key Enabler: Paired Positional Encoding (PPE)** 
PPE duplicates sequences and overwrites dedicated slots (front/back m/2 splits, m=ppe_dim) with positional vectors, preserving 90%+ original meaning. High ppe_dim (384) prioritizes temporal stability, preventing gradient explosions in self-supervision. 

#### Stage 2: High-Difficulty Fine-Tuning – Progressive Overload (512-to-1 Awakening)

With vBERT's foundations laid, Stage 2 applies the 'intentional destruction' of Progressive Overload: expose the primed model to atomic-level extremes via 512-to-1 datasets (token-chunked, all combination trees). **Low ppe_dim=16** shifts to 'meaning insightful' mode, minimizing temporal distortion.

#### Stage 3: Standard Fine-Tuning – Peak Refinement (10-to-1 Mastery)

Stage 3 polishes the awakened model with 10-to-1 datasets (50-token chunks, up to 10 elements), **maintaining ppe_dim=16** for semantic dominance. 

## Benchmark Philosophy: Robust Separation Score (RSS) for RAG Utility

Our Extrapolation Hypothesis demands not just theoretical elegance but empirical proof in real-world utility: Retrieval-Augmented Generation (RAG) systems. Traditional coherence metrics isolate internal consistency; we prioritize **RAG robustness**—ensuring synthesized vectors act as clear signals for relevant memories while drowning irrelevant noise, preventing hallucinations and off-topic retrievals.

### Critical Prerequisite: Semantically Distinct Probes

For benchmarks to measure true synthesis fidelity rather than data artifacts, all test chunks (A, B, C, ..., N) must exhibit **semantic independence**: no pre-existing thematic, stylistic, or topical similarities that could inflate or confound cosine scores. If, e.g., A and C share content from the same domain, synth(ABC) similarity to embed(C) might reflect correlation, not model merit—rendering evaluations invalid.

Mirroring the vBERT pre-training corpus curation:

- **Dataset Source:** Multi-language Wikipedia (wikipedia/wikipedia) editions (e.g., en, ko, fr, de, ja—spanning histories, sciences, arts, etc.).
- **Generation Principle:** Shuffle articles globally, then apply '1 Article = 1 Probe' strictly: each full article processes into one unique chunk/embedding (embed(A)), without sliding windows, excerpts, or intra-article splits.
- **Key Benefits:** Ensures probes originate from disparate 'worlds'—e.g., A from a Korean history article, B from French philosophy—guaranteeing baseline dissimilarities (avg. cosine \<0.1 pre-synthesis). This purity isolates model-induced affinities in RSS gaps, validating RAG signal strength.
- **Implementation:** Custom pipeline similar to `data_pipeline.py` (10k-50k probes per language mix; no Devil randomization to preserve test integrity). Impure probes (e.g., same-language clusters) cap scores at \<5, subverting our 10+ FINESSE triumphs.

This prerequisite is foundational; benchmarks without it risk false positives, undermining the Extrapolation Hypothesis.

### Core Setup
Given a document sequence of chunks A, B, C, D, E with pre-computed embeddings embed(A), ..., embed(E), our model generates cumulative syntheses: synth(AB) from embed(A)+embed(B), synth(ABC) from synth(AB)+embed(C), up to synth(ABCDE). Evaluations probe bidirectional fidelity: how well syntheses 'recognize' their constituents (Top-Down) and how constituents 'recognize' containing syntheses (Bottom-Up).

### Top-Down Evaluation: Synthesis-to-Constituents Fidelity
Tests if a synthesis preserves affinity to its building blocks while rejecting outsiders. E.g., synth(ABC) should exhibit high cosine similarity to embed(A), embed(B), embed(C) (Tier 1: Memory Group X) but low to embed(D), embed(E) (Tier 2: Noise Group Y). This validates 'root retention': the synthesis hasn't forgotten its origins amid merging.

### Bottom-Up Evaluation: Constituents-to-Syntheses Affinity
Complements Top-Down by inverting perspective: a constituent should affinity-match containing syntheses but reject non-containing ones. E.g., embed(C) low similarity to synth(AB) (Noise Y) but high to synth(ABC), synth(ABCD), synth(ABCDE) (Memory X). This ensures 'role persistence': chunks remain anchored in their composite contexts.

### Robust Separation Score (RSS): Quantifying the Gap
For any validator p (synthesis or constituent) against Groups X (expected similar) and Y (expected dissimilar):

1. Compute all pairwise cosines: sims_X = [cos(p, x) for x in X], sims_Y = [cos(p, y) for y in Y].
2. Sort sims_X ascending, sims_Y ascending.
3. Gap = Q1_X (25th percentile of sims_X, weakest expected matches) - Q3_Y (75th percentile of sims_Y, strongest noise).
4. Normalize gap to [0,1] score via min-max scaling (accounting for cosine [-1,1] bounds).

RSS demands the 'barely similar' (Q1_X) decisively outperform the 'most tempting noise' (Q3_Y), creating a 'silent margin' for RAG resilience. Quartiles ensure outlier-robustness: no single perfect match skews results.

### Final FINESSE Scoring: Balance with Penalty
Aggregate td (Top-Down RSS average) and bu (Bottom-Up RSS average) as [0,1] scores:

final = [ (td + bu)/2 - |td - bu| ] × 500

- **Average:** Rewards overall separation.
- **Imbalance Penalty (|td - bu|):** Discourages lopsided specialists; true mastery requires bidirectional consistency.
- **Scaling (×500):** Transforms to intuitive [-1000, +1000] range.

## Acknowledgments

We extend our profoundest thanks to the **intfloat team** and the creators of the `multilingual-e5-base` model. This groundbreaking embedding model was the very foundation of our project: all training datasets were meticulously generated using E5 embeddings, our evaluations were judged against E5 as the gold standard benchmark, and the Synthesizer architecture was specifically designed in symbiotic harmony with E5's multilingual capabilities—making it an organic extension rather than a standalone entity. Without their visionary work in advancing multilingual representation, the Tiny Sequence Merger simply would not exist. Their open-source contribution is the true seed from which our innovations grew.

Built with PyTorch and Transformers. For more on the underlying research, check our project logs.

# vBERT: The Alchemist of Time – Reaching Beyond Meaning's Prison

vBERT is born from the core question: "What if an AI could be shown two disparate narratives (like the coffee and the mother's concern) in chronological order, and then tasked with summarizing the entire unfolding story into a single vector, capturing its temporal flow?"

vBERT is not merely the Sequence Merger; it's the **Vectorized BERT**, a model designed to learn the **sequence of embeddings** itself, rather than just sequences of words. It doesn't treat 'bitter coffee' vector, 'lonely feeling' vector, and 'mother's worry' vector as independent phenomena. Instead, vBERT synthesizes these disparate chunks of meaning within a temporal flow, condensing them into a single, new vector representing the **'atmosphere of that night.'**

Much like individual scenes (vectors) in a film combine to create the movie's overarching emotion (a new vector), vBERT is the **'Alchemist of Time,'** refining temporally ordered embedding sequences into a final 'sequence embedding.'

This marks a new era: **"vBERT empowers AI to finally break free from the prison of meaning, and to understand time itself."**

AI can now move beyond merely predicting the next word or finding the most relevant document. It can grasp how events unfold in sequence, forming the complete tapestry of a story's context.

This is vBERT. The journey continues, and we aim to soon share the models forged with vBERT technology here for all interested.

## Acknowledgments

We extend our profoundest thanks to the **intfloat team** and the creators of the `multilingual-e5-base` model. This groundbreaking embedding model was the very foundation of our project: all training datasets were meticulously generated using E5 embeddings, our evaluations were judged against E5 as the gold standard benchmark, and the Synthesizer architecture was specifically designed in symbiotic harmony with E5's multilingual capabilities—making it an organic extension rather than a standalone entity. Without their visionary work in advancing multilingual representation, the Tiny Sequence Merger simply would not exist. Their open-source contribution is the true seed from which our innovations grew.